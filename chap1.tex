\chapter{Introduction}
\label{chap:intr}

\section{Basics}
\label{sec:basics}

Pole shifting theorem deals with linear differential systems and claims, that it is possible to achieve arbitrary asymptotic behavior. To understand this basic theorem of control theory, we must first describe few basic concepts.

\subsection{Systems of First Order Differential Equations}

\begin{definition}
	A \termdef{system of linear differential equations of order of one with constant coefficients} is a system 
	\begin{align*}
		\dot{x}_1(t)&=a_{1,1}x_1(t)+\ldots+a_{1,n}x_n(t) \\
		&\vdotswithin{=} \\
		\dot{x}_n(t)&=a_{n,1}x_1(t)+\ldots+a_{n,n}x_n(t) 
	\end{align*}
	The system can be written in a matrix form $$\dot{x}(t)=Ax(t)$$ where $x(t)=(x_1(t),\ldots,x_n(t))^T \in \C^n$ is the \termdef{state} of the system. The matrix $A\in \C^{n\times n}$, $A_{i,j}=a_{i,j}$ is a \termdef{fundamental matrix} of the system. The \termdef{starting condition} of the system is the state $x(0)$.
\end{definition}

We will use this representation as it is a very compact way of describing the system.

To express solution of this system in similarly compact matter we will establish a notion of matrix exponential.

\begin{definition}
	Let $X$ be real or complex square matrix. The exponential of $X$, denoted by $e^X$, is the square matrix of same dimensions given by the power series $$e^{X}=\sum _{k=0}^{\infty}\frac{1}{k!}X^{k}$$
	where $X^0$ is defined to be the identity matrix $I$ with the same dimensions as $X$.
\end{definition}

\begin{definition}
	A matrix sequence $\{A^{(k)}\}_{k=0}^\infty$ of $n \times m$ matrices is said to \termdef{converge} to $n\times m$ matrix $A$, denoted by $A^{(k)}\longrightarrow A$, if the following condition is satisfied $$a^{(k)}_{i,j}\xrightarrow{k\rightarrow\infty} a_{i,j}$$
\end{definition}

\begin{lemma}
	Let $\{A^{(k)}\}_{k=0}^\infty$ be a matrix sequence of form $r\times s$. This sequence converges if and only if it satisfies \termdef{Bolzan-Cauchy condition}, that is $$\forall\varepsilon\in\R, \varepsilon>0\quad\exists n_0\in\N\quad\forall m,n\in\N,m\geq n_0, n\geq n_0:||A^{(n)}-A^{(m)}||_F<\varepsilon$$
\end{lemma}

\begin{proof}
	If the sequence converges to matrix $A$, then for each pair $i$, $j$ it from definition of matrix convergence holds $$\forall \varepsilon\in\R,\varepsilon>0\quad\exists n_0\in\N\quad\forall m,n\in\N,m\geq n_0, n\geq n_0:|a^{(n)}_{i,j}-a_{i,j}^{(m)}|<\varepsilon$$ Let $\varepsilon$ be a positive real number. We choose without loss of generality $\varepsilon<1$ For every pair $i$, $j$ we find $n_0$ and we label it $n_{i,j}$. We now put $n_0=\text{min}\{n_{i,j}|i\in\{1,\ldots,r\},j\in\{1,\ldots,s\}\}$. Now for $\forall m,n\in\N,m\geq n_0, n\geq n_0$ we have $$||A^{(n)}-A^{(m)}||_F=\sqrt{\sum^r_{i=1}\sum^s_{j=1}|a^{(n)}_{i,j}-a^{(m)}_{i,j}|^2}<\sqrt{rs\varepsilon^2}=\sqrt{rs}\varepsilon$$

	Conversely, when the condition is satisfied we will use the fact that for positive real numbers $a$, $b$ it holds that $a\leq\sqrt{a^2+b}$. Therefore $$|a^{(n)}_{i,j}-a_{i,j}^{(m)}|\leq\sqrt{\sum^r_{i=1}\sum^s_{j=1}|a^{(n)}_{i,j}-a^{(m)}_{i,j}|^2}<\varepsilon$$ So for any $\varepsilon$ we find $n_0$ such that $\{A^{(k)}\}_{k=0}^\infty$ satisfies BC condition and then we use this $n_0$ for $\{a^{(k)}_{i,j}\}_{k=0}^\infty$.
\end{proof}

\begin{remark}
	A sequence which satisfies Bolzan-Cauchy condition is also called  a \termdef{Cauchy} sequence.
\end{remark}

\begin{lemma}
\label{lem:expprop}
	Let $A$, $B$ and $X$ be real or complex $n\times n$ matrices of same dimensions. Then 
	\begin{enumerate}
		\item $\sum _{k=0}^{\infty}\frac{1}{k!}X^{k}$ converges for any matrix $X$.
		\item $AB = BA \Rightarrow e^{A}B = Be^{A}$
		\item If $R$ is invertible then $e^{R^{-1}XR}=R^{-1}e^AR$
		\item $\frac{d}{dt}e^{tX}=Xe^{tX}$, for $t \in \R$
		\item $AB = BA \Rightarrow e^{A+B} = e^{A}e^B$
	\end{enumerate}
\end{lemma}

\begin{proof}
	\begin{enumerate}
		\sloppy
		\item This can be shown by showing that sequence of partial sums $\{\sum^N_{k=0}\frac{1}{k!}X^k\}_{N=0}^\infty$ is Cauchy. Let $M$, $N\in \N$, then thanks to properties of Frobenius norm it holds
		$$\norm{\sum^M_{k=0}\frac{1}{k!}X^k-\sum^N_{k=0}\frac{1}{k!}X^k}_F=\norm{\sum^M_{k=N+1}\frac{1}{k!}X^k}_F=\sum^M_{k=N+1}\frac{1}{k!}||X||^k_F\xrightarrow{N,M\rightarrow\infty}0$$
		because $\{\sum^N_{k=0}\frac{||X||^k_F}{k!}\}_{N=0}^\infty$ is Cauchy as it is sequence of partial sums of $e^{\norm{X}_F}$, which always converges.

		\item\label{factoring} We know that 
		$$\left(\sum^N_{k=0}\frac{1}{k!}A^k\right)B=\sum^N_{k=0}\frac{1}{k!}A^kB\overset{AB=BA}{=\joinrel=\joinrel=}\sum^N_{k=0}\frac{1}{k!}BA^k=B\left(\sum^N_{k=0}\frac{1}{k!}A^k\right)$$
		Now we have to show that both left and right side converge to $\left(\sum^\infty_{k=0}\frac{1}{k!}A^k\right)B$ and $B\left(\sum^\infty_{k=0}\frac{1}{k!}A^k\right)$ respectively. Let $\varepsilon>0$ be fixed. We want to find such $N_0$ that for every $N\in\N,N\geq N_0$ it holds 
		$$\norm{\left(\sum^\infty_{k=0}\frac{1}{k!}A^k\right)B-\left(\sum^N_{k=0}\frac{1}{k!}A^k\right)B}<\varepsilon$$ 
		Since $\sum^\infty_{k=0}\frac{1}{k!}A^k$ converges we can factor the matrix $B$ out and we can find such $N_0$ that $$\norm{\sum^\infty_{k=0}\frac{1}{k!}A^k-\sum^N_{k=0}\frac{1}{k!}A^k}<\varepsilon$$ Now we can write 
		$$\norm{\left(\sum^\infty_{k=0}\frac{1}{k!}A^k-\sum^N_{k=0}\frac{1}{k!}A^k\right)B}\leq\norm{\sum^\infty_{k=0}\frac{1}{k!}A^k-\sum^N_{k=0}\frac{1}{k!}A^k}\norm{B}<\varepsilon\norm{B}$$ 
		This shows that the converge holds. Together we have 
		$$e^{A}B=\sum^\infty_{k=0}\frac{1}{k!}A^{k}B\stackrel{AB=BA}{=}\sum^\infty_{k=0}\frac{1}{k!}BA^{k}=B\sum^\infty_{k=0}\frac{1}{k!}A^{k}=Be^{A}$$
		
		\item This point follows from the previous point 
		$$e^{R^{-1}XR}=\sum^\infty_{k=0}\frac{1}{k!}(R^{-1}XR)^{k}=\sum^\infty_{k=0}\frac{1}{k!}R^{-1}X^{k}R=R^{-1}\left(\sum^\infty_{k=0}\frac{1}{k!}X^{k}\right)R=R^{-1}e^{X}R$$ 

		\item The series $\sum^\infty_{k=0}\frac{t^k}{k!}X^{k}$ can be understood as simplified expression for system of equations $$f_{i,j}(t)=\sum^\infty_{k=0}\frac{t^k}{k!}a^{(k)}_{i,j}$$ where $a_{i,j}^{(k)}$ is element on postition $(i,j)$ of matrix $X^k$. It is tue that $\abs*{a_{i,j}^{(k)}}\leq \abs*{(\rho n)^k}$ where $n$ is the size of the matrix $X$ and $\rho$ is some fixed real number. It follows $$\abs{\sum^\infty_{k=0}\frac{t^k}{k!}a^{(k)}_{i,j}}\leq\sum^\infty_{k=0}\frac{t^k}{k!}\abs{a^{(k)}_{i,j}}\leq \sum^\infty_{k=0}\frac{(\rho nt)^k}{k!}=e^{\rho nt}$$ Therefore the series converges for any $t\in\R$. We can now differentiate the power series and get $$\frac{d}{dt}\sum^\infty_{k=0}\frac{t^k}{k!}a^{(k)}_{i,j}=\sum^\infty_{k=1}\frac{t^{k-1}}{(k-1)!}a^{(k)}_{i,j}=\sum^\infty_{k=0}\frac{t^{k}}{k!}a^{(k+1)}_{i,j}$$ After expressing all $\frac{d}{dt}f_{i,j}(t)$ again as matrix series and using factoring from point \ref{factoring} we get the desired result $$\frac{d}{dt}e^{tX}=\frac{d}{dt}\sum^\infty_{k=0}\frac{t^k}{k!}X^{k}=\sum^\infty_{k=0}\frac{t^k}{k!}X^{k+1}=X\sum^\infty_{k=0}\frac{t^k}{k!}X^{k}=Xe^{tX}$$
		
		\item Let us write 
		\begin{align*}
			e^{A+B}
			&=\sum^\infty_{k=0}\frac{1}{k!}(A+B)^{k}
			=\sum^\infty_{k=0}\sum^k_{l=0}\binom{k}{l}\frac{1}{k!}A^{l}B^{k-l}
			=\sum^\infty_{k=0}\sum^k_{l=0}\frac{1}{l!(k-l)!}A^{l}B^{k-l}
			\\
			&=\sum^\infty_{k=0}\sum^\infty_{l=0}\frac{1}{k!l!}A^kB^l
			=\sum^\infty_{k=0}\left(\frac{1}{k!}A^k\sum^\infty_{l=0}\frac{1}{l!}B^l\right)
			=\sum^\infty_{k=0}\frac{1}{k!}A^{k}\cdot\sum^\infty_{k=0}\frac{1}{k!}B^{k}
			\\
			&=e^{A}e^B
		\end{align*}
		In the second equation we are using the assumption $AB=BA$. The crucial point is the forth equation in which we basically reorder the equation. The rest is just factoring matrices out of series as done in point \ref{factoring}.
	\end{enumerate}
\end{proof}

Now, using properties from Lemma \ref{lem:expprop} we can see that $\dot{x}(t)=Ax(t)$ is actually solved by $x(0)e^At$. Let us now discuss under what circumstances does this system converge to $\nullvector$. 

Let $A$ be real or complex matrix. Then there is a regular matrix $R\in \C^{n\times n}$ such that $$J=R^{-1}AR$$ is in the Jordan normal form. By substituting $y(t)=R^{-1}x(t)$, which is equivalent with changing the basis of our system, we get 
\begin{align*}
	R\dot{y}(t)&=ARy(t) \\
	\dot{y}(t)&=R^{-1}ARy(t) \\
	\dot{y}(t)&=Jy(t)
\end{align*}
 and therefore the solution is $$y(t)=e^{tJ}y(0)$$ 

We know that every Jordan block $J_{\lambda,n}$ in the matrix $J$ can be decomposed as $J_{\lambda,n}=\lambda I_n+N_n$, $n \in\N$ where $N_n$ is $n \times n$ nilpotent matrix satisfying $n_{i,j}=\delta_{i,j-1}$. For example for $n=4$ we have
\begin{equation*}
	N_4=
	\begin{pmatrix}
		0 & 1 & 0 & 0 \\
		0 & 0 & 1 & 0 \\
		0 & 0 & 0 & 1 \\
		0 & 0 & 0 & 0 
	\end{pmatrix},
	(N_4)^2=
	\begin{pmatrix}
		0 & 0 & 1 & 0 \\
		0 & 0 & 0 & 1 \\
		0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 
	\end{pmatrix},
	(N_4)^3=
	\begin{pmatrix}
		0 & 0 & 0 & 1 \\
		0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 
	\end{pmatrix}
\end{equation*}
It is also true that $(N_n)^k_{i,j}=\delta_{i,j-k}$ and $(N_n)^n=O_{n \times n}$, since every right multiplication by matrix $N$ shifts the multiplied matrix's columns to the right by one column that is it maps matrix $(v_1,\ldots,v_n)$ onto $(o,v_1,\ldots,v_{n-1})$. 

By using Lemma \ref{lem:expprop}, we now for each Jordan block $J_{\lambda,n}$ have $$e^{J_{\lambda,n}t}=e^{(\lambda I + N)t}=e^{\lambda It}e^{Nt}=e^{\lambda t}e^{Nt}$$ Let $\lambda = a+bi$ where $a$,$b \in \R$. Then we have $$e^{J_nt}=e^{at}e^{bit}e^{tN}$$ We know that $|e^{bit}|=1$ and that $$e^{tN}=\sum^\infty_{k=0}\frac{t^k}{k!}N^k=\sum^{n-1}_{k=0}\frac{t^k}{k!}N^k$$ since $(N_n)^n=O_{n \times n}$. Therefore we can see that every element of matrix $e^{tN}$ is a polynom of degree less than $n$. It follows that the whole expression approaches 0 in infinity if $$\lim_{t\to\infty}e^{at}t^{n-1}=0$$ This holds for any $n\in\N$ as long as $a<0$. 

TODO spojit bunky do celej sustavy

Now, since $y(0)$ is a constant vector, we see that $y(t)=e^{Jt}y(0)$ converges to 0 if all the eigenvalues of matrix $A$ are negative in their real parts.

\begin{example}
	Consider higher order differential equation $$x^{(n)}(t)+a_1x^{(n-1)}(t)+\ldots+a_{n-1}x'(t)+a_nx(t)=0$$ where $x(t)\colon\C\rightarrow\C$. This equation can be solved as system of linear differential equations of first order $\dot{z}(t)=Az(t)$ by choosing fundamental matrix $A$ and state vector $z(t)$ as follows
	\begin{equation*}
		A=
		\begin{pmatrix}
			0 & 1 & 0 & \ldots & 0 \\
			0 & 0 & 1 & \ldots & 0 \\
			\vdots & \vdots & \vdots & \ddots & \vdots \\
			0 & 0 & 0 & \ldots & 1 \\
			-a_n & -a_{n-1} & -a_{n-2} & \ldots & -a_1
		\end{pmatrix}
		, z(t)=
		\begin{pmatrix}
			x(t) \\
			x'(t) \\
			\vdots \\
			x^{(n-1)}(t)
		\end{pmatrix}
	\end{equation*}
\end{example}

\subsection{Linear System With A Control}

\begin{definition}
	A \termdef{continuous dynamical linear system with control u} is a system of linear differential equations of first order with constant coefficients in the form $$\dot{x}(t)=Ax(t)+Bu(t)$$ where $A\in\C^{n\times n}$, $B\in\C^{n\times m}$ is a \termdef{control matrix}, $u(t)\in\C^m$ is a \termdef{control vector}. Vector $x(t)\in\C^n$ is called a \termdef{state} of the system.

	Since the system is uniquely defined by the pair $(A,B)$ we also regard this system as $(A,B)$ system.

	TODO pociatocna podmienka
\end{definition}

In a general case, this is called an \termdef{open-loop control} system because the control is not dependent on previous state of the system.

We can imagine this system as follows. The first part of the equation $\dot{x}(t)=Ax(t)$ can be thought of as the model of machine or event that we want to control and $Bu(t)$ as our control mechanism. The $B$ matrix is our ``control board'' and $u(t)$ is us deciding, which levers and buttons we want to push. 

Of course, if we want this system to be self-regulating, we cannot input our own values into $u(t)$ and therefore it has to be calculated from the current state of our system.

\begin{definition}
	Let us have linear differential system with control $u(t)$ defined as follows $$u(t)=Fx(t)$$ where $F\in\C^{m\times n}$ is a \termdef{feedback matrix}. This system is then called a \termdef{closed-loop control} system or a \termdef{linear feedback control} system.
\end{definition}

Typically, we require a feedback control system to stabilize itself back into its stable state after some disturbances. This means that we require that the system converges to some set point. We can assume without loss of generality that this point is the origin of our state space i.e. all the possible states of $x(t)$. This can be achieved by transforming the system into different basis.

The feedback control system can be expressed as a first order linear differential system $$\dot{x}(t)=Ax(t)+BFx(t)=(A+BF)x(t)$$ TODO As discussed in the first section, we now know that the system converges to $\nullvector$ if all of the eigenvalues of matrix $A+BF$ are negative in their real parts. 

Therefore the system can stabilize itself if we find such matrix $F \in \C^{n \times n}$ that $A+BF$ have all eigenvalues with negative real part. This requirement can be expressed through characteristic polynomial of matrix $A+BF$.

\begin{definition}
	Let $A$ be a $n\times n$ matrix. Then the \termdef{characteristic polynomial} of A, denoted by $\chi_A$, is defined as $$\chi_A(s)=\textnormal{det}(sI_n-A)$$
\end{definition}

Through our observations we got to a conclusion that we need to satisfy $$\chi_{A+BF}=(x-\lambda_1)(x-\lambda_2)\cdots(x-\lambda_n)$$ where $\lambda_1,\lambda_2,\ldots,\lambda_n \in \C$ have their real parts negative. This leads to an important definition.

\begin{definition}
    Let $\K$ be a field and let $A \in \K^{n \times n}$, $B \in \K^{n \times m}$, $n,m \in \N$. We say that polynomial $\chi$ is \termdef{assignable} for the pair $(A,B)$ if there exists such matrix $F\in\K^{m \times n}$ that $$\chi_{A+BF}=\chi$$
\end{definition}

The pole shifting theorem states, that if $A$ and $B$ are ``sensible'' in a sense that we will discuss in the next section, then arbitrary polynomial $\chi$ of dimension that depends on how ``sensible'' $A$ and $B$ are, can be assigned to the pair $(A,B)$. It also claims that it is immaterial over what field $A$ and $B$ are.

\section{Controllable pairs}

In this section we will establish the notion of controllability. We will first explain this concept for \textit{discrete-time systems} and then we will show that the requirement for controllability for \textit{discrete-time systems} also holds for \textit{continuos-time systems}.

\subsection{Discrete-time systems}

TODO definicia diskretneho systemu, stavov a reach

\begin{definition}
	
\end{definition}

States that we can reach in set number of iterations in a open-loop control \textit{discrete-time systems} can be derived as follows. From state $x_k$ and control vector $u_k$ is the next state $x_{k+1}$ computed by equation $$x_{k+1}=Ax_k+Bu_k$$ where $\K$ is a field, $A\in\K^{n\times n}$ and $B\in\K^{n\times m}$. The starting condition is $x_0=\textbf{o}$ and we can choose arbitrary $u_k$. Then, for $k=0$ we have $$x_1=Ax_0+Bu_0=Bu_0 \in \text{Im}B.$$ For $k=1$ we get $$x_2=Ax_1+Bu_1=ABu_0+Bu_1\in\text(AB|B).$$ It is clear, that $$x_k\in\text{Im}(A^{k-1}B|\ldots|AB|B).$$ We can observe that $\text{Im}(B|AB|\ldots|A^kB) \subseteq \text{Im}(B|AB|\ldots|A^{k+1}B)$. From Cayley-Hamilton theorem we know that $\chi_A(A)=O_{n\times n}$. That means that $A^n$ can be expressed as linear combination of matrices $\{I,A,\ldots,A^{n-1}\}$ or equivalently that $A^nB$ can be expressed as linear combination of matrices $\{B,AB,\ldots,A^{n-1}B\}$. We now see that $$\text{Im}(B|AB|\ldots|A^{n-1}B)=\text{Im}(B|AB|\ldots|A^{n-1}B|A^nB).$$ Therefore all the states we could ever reach are already in space $$\text{Im}(B|AB|\ldots|A^{n-1}B)$$

\begin{definition}
	Let $\K$ be a field and let $A \in \K^{n \times n}$, $B \in \K^{n \times m}$, $n,m \in \N$. We define \termdef{reachable space} $\mathcal{R}(A,B)$ of the pair $(A,B)$ as $\text{Im}(B|AB|\ldots|A^{n-1}B)$.
\end{definition}

We have seen that by left multiplying $\mathcal{R}(A,B)$ by $A$ we get the same subspace. This leads to an important property of some subspaces.

\begin{definition}
	Let $V$ be a vector space, $W$ be its subspace and $f$ be a mapping from $V$ to $V$. We call $W$ an \termdef{invariant subspace} of $f$ if $f(W)\subseteq W$.

	We also say that $W$ is \termdef{$f$-invariant}. When $f=f_A$ for some matrix $A$ we also shortly say that W is \termdef{$A$-invariant}.
\end{definition}

\begin{remark}
	\label{rem:reachinv}
	$\mathcal{R}(A,B)$ is a $A$-invariant subspace.
\end{remark} 

The maximum dimension of $\mathcal{R}(A,B)$ is, of course, $n$. This leads us to important property of pair $(A,B)$, where we want to able to get the ``machine'' into any state in state space by controlling it with our control matrix $B$. Therefore we desire that $\mathcal{R}(A,B)=\K^n$. The equivalent condition is $\text{dim}\mathcal{R}(A,B)=n$.

\begin{definition}
	Let $\K$ be a field and let $A \in \K^{n \times n}$, $B \in \K^{n \times m}$, $n,m \in \N$. The pair $(A,B)$ is \termdef{controllable} if $\textnormal{dim}\mathcal{R}(A,B)=n$.
\end{definition}

\subsection{Continuous-time systems}

TODO opravit dokaz + doplnit lemma 1, idealne aj reach nejak

We will now show that the condition for \textit{discrete-time systems} also characterizes \textit{continuous-time systems}. For this we have to express solution of such system using matrices $A^iB$ for $i\in \N_0$. 

We utilize matrix exponential in solving system of inhomogeneous linear system $\dot{x}(t)=Ax(t)+Bu(t)$. By left multiplying it by $e^{-tA}$ we get
\begin{align*}
	e^{-tA}\dot{x}(t)-e^{-tA}Ax(t) &=e^{-tA}Bu(t) \\
	\frac{d}{dt} (e^{-tA}x(t)) &=e^{-tA}Bu(t) 
\end{align*}
Note, we used $-AA=A(-A)\Rightarrow e^{-tA}A=Ae^{-tA}$. After integrating both sides with respect to $t$ on interval $(t_0,t_1)$ we have 
\begin{align*}
	[e^{-tA}x(t)]^{t_1}_{t_0}&=\int^{t_1}_{t_0}e^{-tA}Bu(t)dt \\
	e^{-t_1A}x(t_1)-e^{-t_0A}x(t_0)&=\int^{t_1}_{t_0}e^{-tA}Bu(t)dt \\
	x(t_1)&=e^{(t_1-t_0)A}x(t_0)+\int^{t_1}_{t_0}e^{(t_1-t)A}Bu(t)dt
\end{align*}

Now it is clear that in system where $x(0)=\nullvector$ can every state in time $t\in \R^+$ be expressed as $$x(t)=\int^t_0 e^{(t-s)A}Bu(s)ds$$

\begin{lemma}
\label{lem:}
\end{lemma}

\begin{theorem}
	The $n$-dimensional continuos-time linear system is controllable, meaning that $x(t)$ can be equal to any vector in $\K^n$, if and only if $\text{dim}\mathcal{R}(A,B)=n$.
\end{theorem}

\begin{proof}
	From discussion above and Lemma \ref{lem:expprop} we have 
	\begin{align*}
		x(t)&=\int^t_0e^{(t-s)A}Bu(s)ds
		=\int^t_0\sum^\infty_{k=0}\frac{(t-s)^k}{k!}A^kBu(s)ds 
		\\
		&=\sum^\infty_{k=0}A^kB\int^t_0\frac{(t-s)^k}{k!}u(s)ds
		=\sum^\infty_{k=0}A^kBv_k(t)
	\end{align*}
	where $v_k(t)=\int^t_0\frac{(t-s)^k}{k!}u(s)ds$ is a vector of length $m$.
	
	We can see that $$x(t) \in \text{Im}(B|AB|\ldots|A^kB|\ldots)\subseteq \text{Im}(B|AB|\ldots|A^{n-1}B)=\mathcal{R}(A,B)$$ 
	
	If the system is controllable then $x(t)$ can be equal to any of the vectors of an arbitrary basis of $\K^n$. Therefore we know that $n$ linearly independent vectors belong into $\mathcal{R}(A,B)$ and naturally it follows $\text{dim}\mathcal{R}(A,B)=n$.

	Conversely, if dimension of reachable space is equal to $n$ we then have $x(t)\in\mathcal{R}(A,B)=\C^n$, therefore the system is controllable.
\end{proof}

\subsection{Decomposition theorem}

\begin{lemma}
	\label{lem:invsubspc}
	Let $W$ be an invariant subspace of linear mapping $f\colon V \rightarrow V$. Then there exists a basis $C$ of $V$ such that 
	\begin{equation*}
		[f]^C_C=
		\begin{pmatrix}
			F_1 & F_2 \\
			0   & F_3 
		\end{pmatrix}
	\end{equation*}
	where $F_1$ is $r\times r$, $r=\text{dim}W$.
\end{lemma}

\begin{proof}
	Let $(w_1,\ldots,w_r)$ be an arbitrary basis of subspace $W$. We complete this sequence into basis $C$ of $V$ with vectors $v_1,\ldots,v_{n-r}$ where $n=\text{dim}V$, thus $C=(w_1,\ldots,w_r,v_1,\ldots,v_{n-r})$. We know that $$[f]^C_C=([f(w_1)]_C,\ldots,[f(w_r)]_C,[f(v_1)]_C,\ldots,[f(v_{n-r})]_C)$$ Since $W$ is $A$-invariant subspace, it holds that $f(w_i)\in W$ and therefore, thanks to our choice of the basis $C$, we get the desired form.
\end{proof}

If $(A,B)$ are not controllable then there exists subspace of our state space that is not affected by our input. This can be shown using following theorem.

\begin{theorem}
	\label{theorem:decomp}
	Assume that $(A,B)$ is not controllable. Let $\text{dim}\mathcal{R}(A,B)=r<n$. Then there exists invertible $n\times n$ matrix $T$ over $\K$ such that the matrices $\widetilde{A}:=T^{-1}AT$ and $\widetilde{B}:=T^{-1}B$ have the block structure 
	\begin{equation*}
		\widetilde{A}=
		\begin{pmatrix}
			A_1 & A_2 \\
			0   & A_3 
		\end{pmatrix}
		\qquad
		\widetilde{B}=
		\begin{pmatrix}
			B_1  \\
			0
		\end{pmatrix}
	\end{equation*}
	where $A_1$ is $r \times r$ and $B_1$ is $r \times m$.
\end{theorem}

\begin{proof}
	We know that $\mathcal{R}(A,B)$ is $A$-invariant subspace (Remark \ref{rem:reachinv}). Using Lemma \ref{lem:invsubspc} on matrix mapping $f_A$ we get a basis $C$ for which it holds $$[f_A]^C_C=[\text{id}]^K_C[f_A]^K_K[\text{id}]^C_K=[\text{id}]^K_CA[\text{id}]^C_K$$ is in a block triangular form. By putting $T=[\text{id}]^C_K=C$ we get $\widetilde{A}=[f_A]^C_C$ which is in the desired form.

	Consider now matrix mapping $f_B$. We have $$\widetilde{B}=TB=[\text{id}]^{K_n}_C[f_B]^{K_m}_{K_n}=[f_B]^{K_m}_C=([f_B(e_1)]_C,\ldots,[f_B(e_m)]_C)$$ Since $f_B(e_i)$ is $i$-th column of matrix $B$ and trivially from definition it holds $\text{Im}(B)\subseteq \mathcal{R}(A,B)$ we get the desired form of $\widetilde{B}$.
\end{proof}

We achieved the new form of matrices $A$ and $B$ by changing the basis of our state space. We now define the relation between $(A,B)$ and $(\widetilde{A},\widetilde{B}).$

\begin{definition}
	Let $(A,B)$ and $(\widetilde{A},\widetilde{B})$ be pairs as in Theorem \ref{theorem:decomp} above. Then $(A,B)$ \termdef{is similar to} $(\widetilde{A},\widetilde{B})$, denoted $$(A,B) \sim (\widetilde{A},\widetilde{B})$$ if there exists invertible matrix $T$ for which it holds that $$\widetilde{A}=T^{-1}AT\quad and\quad\widetilde{B}=T^{-1}B$$
\end{definition}

We can interpret the decomposition as follows. Consider our system $\dot{x}(t)=Ax(t)+Bu(t)$. By changing the basis by putting $x(t)=Ty(t)$ we get $$T\dot{y}(t)=ATy(t)+Bu(t)$$ which we can write as $$\dot{y}(t)=T^{-1}ATy(t)+T^{-1}Bu(t)=\widetilde{A}y(t)+\widetilde{B}u(T)$$ Which gives us 
\begin{alignat*}{2}
	\dot{y}_1(t)&=A_1y_1(t)+&A_2y_2(t)&+B_1u_1(t) \\
	\dot{y}_2(t)&=&A_3y_2(t)&
\end{alignat*}
where $y_1(t)$ is composed of the first $r$ elements of $y(t)$, $y_2(t)$ is composed of the other $n-r$ elements of $y(t)$ and $u_1(t)$ is composed of the first $r$ elements of $u(t)$. Since $\dot{y}_2(t)$ does not depend on control vector we cannot change these last $n-r$ coordinates of $y(t)$.

It is also true that $(A_1,B_1)$ from Theorem \ref{theorem:decomp} is a controllable pair which we will state in a lemma.

\begin{lemma}
	The pair $(A_1,B_1)$ is controllable.
\end{lemma}

\begin{proof}
	We know that $\text{dim}\mathcal{R}(A,B)=r$. We desire $\text{dim}\mathcal{R}(A_1,B_1)=r$. We will show that $\mathcal{R}(\widetilde{A},\widetilde{B})=\mathcal{R}(A,B)$ and that each vector in $\mathcal{R}(\widetilde{A},\widetilde{B})$ has its last $n-r$ elements equal to 0 and that $\mathcal{R}(\widetilde{A},\widetilde{B})$ restricted on its first $r$ coordinates is equal to $\mathcal{R}(A_1,B_1)$. 
	\begin{align*}
		\mathcal{R}(\widetilde{A},\widetilde{B})&=\text{Im}(\widetilde{A}^{n-1}\widetilde{B}|\ldots|\widetilde{A}\widetilde{B}|\widetilde{B}) \\
		&=\text{Im}((T^{-1}AT)^{n-1}T^{-1}B|\ldots|T^{-1}ATT^{-1}B|T^{-1}B) \\
		&=\text{Im}(T^{-1}A^{n-1}B|\ldots|T^{-1}AB|T^{-1}B) \\
		&=\{(T^{-1}A^{n-1}B|\ldots|T^{-1}AB|T^{-1}B)\cdot v | v \in \K^{n\cdot m}\} \\
		&=\{T^{-1}(A^{n-1}B|\ldots|AB|B)\cdot v | v \in \K^{n\cdot m}\} \\
		&=T^{-1}\cdot\{(A^{n-1}B|\ldots|AB|B)\cdot v | v \in \K^{n\cdot m}\} \\
		&=T^{-1}\cdot(\text{Im}(A^{n-1}B|\ldots|AB|B)) \\
		&=T^{-1}\cdot(\mathcal{R}(A,B))
	\end{align*}
	where by $\cdot\colon\K^{n\times m}\times V\rightarrow W$ where $V$, $W$ are vector spaces is defined as $A\cdot V=\{Av|v\in V\}$.

	Since $T$ is an invertible matrix, which preserves linear independence, we have $$\text{dim}\mathcal{R}(\widetilde{A},\widetilde{B})=\text{dim}(T^{-1}\mathcal{R}(A,B))=\text{dim}(\mathcal{R}(A,B))=r$$

	Now let us focus on the structure of $\mathcal{R}(\widetilde{A},\widetilde{B})$: We know that last $n-r$ rows of $\widetilde{B}$ are $\nullvector$. Also because of structure of $\widetilde{A}$ we have for an arbitrary matrix $X\in\K^{r\times m}$ that 
	\begin{equation*}
		\widetilde{A}
		\begin{pmatrix}
			X \\
			0
		\end{pmatrix}
		=
		\begin{pmatrix}
			A_1 & A_2 \\
			  0 & A_3
		\end{pmatrix}
		\begin{pmatrix}
			X \\
			0
		\end{pmatrix}
		=
		\begin{pmatrix}
			A_1X \\
			0
		\end{pmatrix}
	\end{equation*}
	where again are the last $n-r$ rows equal to $\nullvector$. Therefore we see that for any positive integer $k$ we have 
	\begin{equation*}
		\widetilde{A}^k\widetilde{B}=
		\begin{pmatrix}
			A_1^{k}B_1 \\
			0
        \end{pmatrix}
        ,A_1^kB_1\in\K^{r\times r}
    \end{equation*}
    It follows
    \begin{equation*}
        \mathcal{R}(\widetilde{A},\widetilde{B})=
        \begin{pmatrix}[c|c|c|c]
            \begin{pmatrix}
                A_1^{n-1}B_1 \\
                0 
            \end{pmatrix}
            & \ldots &
            \begin{pmatrix}
                A_1B_1 \\
                0 
            \end{pmatrix}
            &
            \begin{pmatrix}
                B_1 \\
                0 
            \end{pmatrix}
        \end{pmatrix}
    \end{equation*}
	
	From Cayle-Hemilton theorem we therefore again have that the restriction to first $r$ coordinates (those which are not 0) of $\mathcal{R}(\widetilde{A},\widetilde{B})$ are equal to $\mathcal{R}(A_1,B_1)$. Finally, it follows that $$\text{dim}\mathcal{R}(A_1,B_1)=\text{dim}\mathcal{R}(\widetilde{A},\widetilde{B})=\text{dim}\mathcal{R}(A,B)=r$$
\end{proof}

Now we can see that the decomposition from Theorem \ref{theorem:decomp} decomposes the matrix $A$ into ``controllable'' and ``uncontrollable'' parts $A_1$ and $A_3$ respectively.

We also see that 
\begin{align*}
	\chi_{\widetilde{A}}&=\text{det}(sI-\widetilde{A})=\text{det}(sI-T^{-1}AT) \\
	&=\text{det}(sT^{-1}IT-T^{-1}AT)=\text{det}(T^{-1}(sI-A)T) \\
	&=(\text{det}T)^{-1}\text{det}(sI-A)\text{det}T=\text{det}(sI-A) \\
	&=\chi_A
\end{align*}
therefore it holds $$\chi_A=\chi_{A_1}\chi_{A_3}$$ 

\begin{definition}
	The characteristic polynomial of matrix $A$ splits into \termdef{controllable} and \termdef{uncontrollable parts} with respect to pair $(A,B)$ which we denote by $\chi_c$ and $\chi_u$ respectively. We define these polynomials as $$\chi_c=\chi_{A_1} \qquad \chi_u=\chi_{A_3}$$ In case $r=0$ we put $\chi_c=1$ and in case $r=n$ we put $\chi_u=1$.
\end{definition}

For this definition to be correct we need to show that polynomials $\chi_{A_1}$ and $\chi_{A_3}$ are not dependent on the choice of basis on $\mathcal{R}(A,B)$. Since $\chi_{A_3}=\chi_A/\chi_{A_1}$, it is enough to show that $\chi_{A_1}$ is independent of the choice.

\begin{lemma}
	$\chi_c$ is independent of the choice of basis on $\mathcal{R}(A,B)$.
\end{lemma}

\begin{proof}
	From definition we have $\chi_c=\chi_{A_1}$ where $A_1$ is some matrix for a specific decomposition of matrix $A$ thanks to basis $C$ used in a proof of Theorem \ref{theorem:decomp}. Consider different basis $D$ which suffices the conditions from the said proof. We denote the $r \times r$ matrix in the top left corner of the resulting matrix by $X_1$. 

	Suppose $\chi_{X_1}\neq \chi_{A_1}$. Then we can write 
	\begin{align*}
		\chi_{X_1}&\neq\text{det}(sI-X_1)=\text{det}(sI-T^{-1}AT) \\
		&=\text{det}(sT^{-1}IT-T^{-1}AT)=\text{det}(T^{-1}(sI-A)T) \\
		&=(\text{det}T)^{-1}\text{det}(sI-A)\text{det}T=\text{det}(sI-A) \\
		&=\chi_A
	\end{align*}

	TODO plánujem tie matice X1 a A1 rozšíriť nulami na veľkosť n x n a potom ich charakteristicke polynomy prenasobit determinantom prechodnej matice medzi C a D a takisto inverznou hodnotou tohoto determinantu z coho ziskam nakoniec ze chiX1 sa nerovna chiX1 co je spor
\end{proof}