\chapter{Introduction}
\label{chap:intr}

\section{Basics}
\label{sec:basics}

Pole shifting theorem deals with linear differential systems and claims, that it is possible to achieve arbitrary asymptotic behavior. To understand this basic theorem of control theory, we must first describe few basic concepts.

\subsection{Systems of First Order Differential Equations}

\begin{definition}
	Let us have a system of linear differential equations of order of one with constant coefficients. Then a \termdef{matrix differential equation} for this system is in the form $$\dot{x}(t)=Ax(t)$$ 
\end{definition}

We will use this representation as it a very compact way of describing the system.

To express solution of this system in similarly compact matter we will establish a notion of matrix exponential.

\begin{definition}
	Let $X$ be real or complex square matrix. The exponential of $X$, denoted by $e^X$, is the square matrix of same dimensions given by the power series $$e^{X}=\sum _{k=0}^{\infty}\frac{1}{k!}X^{k}$$
	where $X^0$ is defined to be the identity matrix $I$ with the same dimensions as $X$.
\end{definition}

\begin{remark}
\label{rem:expprop}
	Let $A$, $B$ and $X$ be real or complex square matrices of same dimensions. Then 
	\begin{enumerate}
		\item $e^X$ converges for any matrix $X$.
		\item $\frac{d}{dt}e^{Xt}=Xe^{Xt}$, for $t \in \R$
		\item $AB = BA \Rightarrow e^{A+B} = e^{A}e^B$
		\item $AB = BA \Rightarrow e^{A}B = Be^{A}$
		\item If $R$ is invertible then $e^{R^{-1}XR}=R^{-1}e^AR$
	\end{enumerate}
\end{remark}

\begin{proof}
	\begin{enumerate}
		\item We want to show that the partial sums $S_N=\sum^N_{k=0}\frac{1}{k!}X^k$ converge to $e^X$. This can be shown by choosing any matrix norm satisfying $||AB||<||A||\cdot||B||$ and writing $$||e^X-S_N||=||\sum^\infty_{k=N+1}\frac{1}{k!}X^k||=\sum^\infty_{k=N+1}\frac{1}{k!}||X||^k$$ where right side approaches zero since $\sum^\infty_{k=0}\frac{||A||^k}{k!}$ converges.

		\item Thanks to the convergence we have $$\frac{d}{dt}e^{Xt}=\frac{d}{dt}\sum^\infty_{k=0}\frac{t^k}{k!}X^{k}=\sum^\infty_{k=0}\frac{t^k}{k!}X^{k+1}=X\sum^\infty_{k=0}\frac{t^k}{k!}X^{k}=Xe^{Xt}$$

		\item Let us write $$e^{A+B}=\sum^\infty_{k=0}\frac{1}{k!}(A+B)^{k}=\sum^\infty_{k=0}\sum^k_{l=0}\binom{k}{l}\frac{1}{k!}A^{l}B^{k-l}=\sum^\infty_{k=0}\frac{1}{k!}A^{k}\cdot\sum^\infty_{k=0}\frac{1}{k!}B^{k}=e^{A}e^B$$ In the second equation we are using the assumption $AB=BA$ and in the third equation the Cauchy product formula is used.

		\item From definition follows $$e^{A}B=\sum^\infty_{k=0}\frac{1}{k!}A^{k}B\stackrel{AB=BA}{=}\sum^\infty_{k=0}\frac{1}{k!}BA^{k}=B\sum^\infty_{k=0}\frac{1}{k!}A^{k}=Be^{A}$$ 
		
		\item We have $$e^{R^{-1}XR}=\sum^\infty_{k=0}\frac{1}{k!}(R^{-1}XR)^{k}=\sum^\infty_{k=0}\frac{1}{k!}R^{-1}X^{k}R=R^{-1}(\sum^\infty_{k=0}\frac{1}{k!}X^{k})R=R^{-1}e^{X}R$$ 
	\end{enumerate}
\end{proof}

Now, using properties from Remark \ref{rem:expprop} we can see that $\dot{x}(t)=Ax(t)$ is actually solved by $x(0)e^At$. Let us now discuss under what circumstances does this system converge to $\nullvector$. 

Let $A$ be real or complex matrix. Then we have $$J=R^{-1}AR$$ where $J$ is a matrix in the Jordan normal form. Therefore we can write $$\dot{x}(t)=RR^{-1}ARR^{-1}x(t)=RJR^{-1}x(t)$$ It follows that $$R^{-1}\dot{x}(t)=\dot{(R^{-1}x)}=J R^{-1}x(t)$$ By substituting $y(t)=R^{-1}x(t)$, which is equivalent with changing the basis of our system, we get $$\dot{y}(t)=Jy(t)$$ and therefore the solution is $$y(t)=e^{Jt}y(0)$$ 

We know that every Jordan block $J_{\lambda,n}$ in the matrix $J$ can be decomposed as $J_{\lambda,n}=\lambda I_n+N_n$, $n \in\N$ where $N_n$ is $n \times n$ nilpotent matrix satisfying $n_{i,j}=\delta_{i,j-1}$. For example for $n=4$ we have
\begin{equation*}
	N_4=
	\begin{pmatrix}
		0 & 1 & 0 & 0 \\
		0 & 0 & 1 & 0 \\
		0 & 0 & 0 & 1 \\
		0 & 0 & 0 & 0 
	\end{pmatrix}
\end{equation*}
It is also true that $(N_n)^n=\textbf{O}$, since every right multiplication by matrix $N$ shifts the multiplied matrix's columns to the right by one column. 

By using Remark \ref{rem:expprop}, we now for each Jordan block $J_{\lambda,n}$ have $$e^{J_{\lambda,n}t}=e^{(\lambda I + N)t}=e^{\lambda It}e^{Nt}=e^{\lambda t}e^{Nt}$$ Let $\lambda = a+bi$ where $a$,$b \in \R$. Then we have $$e^{J_nt}=e^{at}e^{bit}e^{Nt}$$ We know that $|e^{bit}|=1$ and that $$e^{Nt}=\sum^{n-1}_{k=0}\frac{t^k}{k!}N^k$$ so the highest power of $t$ in the matrix is $n-1$. Therefore, we can see that the whole expression approaches 0 in infinity if $$\lim_{t\to\infty}e^{at}t^{n-1}=0$$ This holds for any $n\in\N$ as long as $a<0$. 

Now, since $y(0)$ is a constant vector, we see that $y(t)=e^{Jt}y(0)$ converges to 0 if all the eigenvalues of matrix $A$ are negative in their real parts.

\subsection{Omg}

The state of linear system can be represented by system of $n$ differential equations $$\dot{x}(t)=Ax(t)+Bu(t),$$ where $x(t)$ is the $n$-dimensional state vector $(\varphi(t),\dot{\varphi}(t),\ldots,\varphi^{(n-1)}(t))^T$, $u(t)$ is the $m$-dimensional \textit{input} or \textit{control vector}, $A \in \C ^{n\times n}$ is matrix of coefficients and $B\in \C ^{m\times n}$ is \textit{input} or \textit{control matrix}. The whole system is called \textit{n-dimensional system}. The control vector $u(t)$ is acquired from $x(t)$ by multiplying a control matrix $F\in \C ^{m\times n}$ by $x(t)$. All the possible states of $x(t)$ create \textbf{state space} which is usually equal to $\C^n$. 

We can imagine this system as follows. The first part of the equation $\dot{x}(t)=Ax(t)$ can be thought of as the model of machine or event that we want to control and $Bu(t)$ as our control mechanism. The $B$ matrix is our ``control board'' and $u(t)$ is us deciding, which levers and buttons we want to push. Of course, if we want this system to be self-regulating, we cannot input our own values into $u(t)$ and therefore it has to be calculated from the current state of our system. Thus $u(t)=Fx(t)$. The whole system can then be rewritten as $$\dot{x}(t)=Ax(t)+BFx(t)=(A+BF)x(t).$$ If $A+BF$ is a diagonalizable matrix, then we have $$\Lambda=R^{-1}(A+BF)R,$$ where $R \in \C ^{n \times n}$ is an invertible matrix and $\Lambda \in \C ^{n \times n}$ is a diagonal matrix. We can write that $$\dot{x}(t)=RR^{-1}(A+BF)RR^{-1}x(t)=R\Lambda R^{-1}x(t),$$ it follows, that $$R^{-1}\dot{x}(t)=\dot{(R^{-1}x)}=\Lambda R^{-1}x(t).$$ By substituting $y(t)=R^{-1}x(t)$ we get $$\dot{y}(t)=\Lambda y(t).$$ This equation represents system of simple linear differential equations. If we denote elements on diagonal of $\Lambda$ by $\lambda_1,\lambda_2,\ldots,\lambda_n$ the resulting equations are  
\begin{align*}
  \dot{y}_1(t)&=\lambda_1y_1(t) \\
  \dot{y}_2(t)&=\lambda_2y_2(t) \\
  &\vdotswithin{=} \\
  \dot{y}_n(t)&=\lambda_ny_n(t) 
\end{align*}
Solution to each of these equations is in the form 
$$y_k(t)=y_k(0)e^{\lambda_kt}, k\in\{1,2,\ldots,n\}.$$
Let $\lambda_k=a_k+b_ki$ where $a_k,b_k\in \R$, then 
$$y_k(0)e^{\lambda_kt}=y_k(0)e^{a_kt}e^{b_kit}.$$ 
We know, that $|e^{b_kit}|=1$ and that $y_k(0)$ is a constant, so the crucial part is $e^{a_kt}$. This converges to 0 if and only if $a_k$ is negative. Therefore we can stabilize our ``machine'' if we find such matrix $F \in \C^{n \times n}$ that $A+BF$ is diagonalizable with distinct eigenvalues (then we are sure that $A+BF$ is diagonalizable) with negative real part. This can be expressed through characteristic polynomial of matrix $A+BF$. We will denote characteristic polynomial of a matrix $A$ by $\chi_A$. Through our observations we got to a conclusion that we need to satisfy $$\chi_{A+BF}=(x-\lambda_1)(x-\lambda_2)\cdots(x-\lambda_n),$$ where $\lambda_1,\lambda_2,\ldots,\lambda_n \in \C$ are different from one another and their real parts are negative. This leads to an important definition.

\begin{definition}
    Let $\K$ be a field and let $A \in \K^{n \times n}$, $B \in \K^{n \times m}$, $n,m \in \N$. We say that polynomial $\chi$ is \termdef{assignable} for the pair $(A,B)$ if there exists such matrix $F\in\K^{m \times n}$ that $$\chi_{A+BF}=\chi$$
\end{definition}

The pole shifting theorem states, that if $A$ and $B$ are ``sensible'' in a sense that we will discuss in the next section, then arbitrary polynomial $\chi$ of dimension that depends on how ``sensible'' $A$ and $B$ are, can be assigned to the pair $(A,B)$. It also claims that it is immaterial over what field $A$ and $B$ are.

\section{Controllable pairs}

In this section we will establish the notion of controllability. We will first explain this concept for \textit{discrete-time systems} and then we will show that the requirement for controllability for \textit{discrete-time systems} also holds for \textit{continuos-time systems}.

\subsection{Discrete-time systems}

States that we can reach in set number of iterations in a \textit{discrete-time systems} can be derived as follows. From state $x_k$ and control vector $u_k$ is the next state $x_{k+1}$ computed by equation $$x_{k+1}=Ax_k+Bu_k$$ where $\K$ is a field, $A\in\K^{n\times n}$ and $B\in\K^{n\times m}$. The starting condition is $x_0=\textbf{o}$ and we can choose arbitrary $u_k$. Then, for $k=0$ we have $$x_1=Ax_0+Bu_0=Bu_0 \in \text{Im}B.$$ For $k=1$ we get $$x_2=Ax_1+Bu_1=ABu_0+Bu_1\in\text{Im}(AB|B).$$ It is clear, that $$x_k\in\text{Im}(A^{k-1}B|\ldots|AB|B).$$ We can observe that $\text{Im}(B|AB|\ldots|A^kB) \subseteq \text{Im}(B|AB|\ldots|A^{k+1}B)$. Then, from Cayley-Hamilton theorem we know that $$\text{Im}(B|AB|\ldots|A^{n-1}B)=\text{Im}(B|AB|\ldots|A^{n-1}B|A^nB).$$ Therefore all the states we could ever reach are already in space $$\text{Im}(B|AB|\ldots|A^{n-1}B).$$

\begin{definition}
	Let $\K$ be a field and let $A \in \K^{n \times n}$, $B \in \K^{n \times m}$, $n,m \in \N$. We define \termdef{reachable space} $\mathcal{R}(A,B)$ as $\text{Im}(B|AB|\ldots|A^{n-1}B)$.
\end{definition}

\begin{definition}
	Let $V$ be a vector space, $W$ be its subspace and $f$ be a mapping from $V$ to $V$. We call $W$ an invariant subspace of $f$ if $f(W)\subseteq W$. 

	We also say that $W$ is $f$-invariant.
\end{definition}

\begin{lemma}
	\label{lem:invmatrix}
	Let $W$ be an invariant subspace of linear mapping $f\colon V \rightarrow V$. Then there exists a basis $C$ of $V$ such that 
	\begin{equation*}
		[f]^C_C=
		\begin{pmatrix}
			F_1 & F_2 \\
			0   & F_3 
		\end{pmatrix}
	\end{equation*}
	where $F_1$ is $r\times r$, $r=\text{dim}W$.
\end{lemma}

\begin{proof}
	We have $$[f]^C_C=[\text{id}]^K_C [f]^K_K [\text{id}]^C_K=([\text{id}]^C_K)^{-1} [f]^K_K [\text{id}]^C_K$$ where $[id]^C_K$ is a transition matrix from basis $C$ to canonical basis $K$. Let $(w_1,\ldots,w_r)$ be an arbitrary basis of subspace $W$. We complete this sequence into basis of $V$ with vectors $v_1,\ldots,v_{n-r}$ where $n=\text{dim}V$. We now put $$C=(w_1,\ldots,w_r,v_1,\ldots,v_{n-r})$$ Since $W$ is $f$-invariant we know that $[f]^K_K [\text{id}]^C_K = (u_1,\ldots,u_r,z_1,\ldots,z_{n-r})$, where $u_i\in W$ and $z_i\in V$. Now, by left multiplying this result by $[\text{id}]^K_C$ we get the matrix $([u_1]_C,\ldots,[u_r]_C,[z_1]_C,\ldots,[z_{n-r}]_C)$. Thanks to our choice of basis $C$ can any vector $u_i$ be expressed as a linear combination of vectors $(w_1,\ldots,w_r)$ and therefore we now have the desired form.
\end{proof}

From discussion above it is clear that for arbitrary $v\in\mathcal{R}(A,B)$ we have $Av\in\mathcal{R}(A,B)$. This property is called $A$-\textit{invariance}.

The maximum dimension of $\mathcal{R}(A,B)$ is, of course, $n$. This leads us to important property of pair $(A,B)$, where we want to able to get the ``machine'' into any state in state space by controlling it with our control matrix $B$. Therefore we desire that $\mathcal{R}(A,B)=\K^n$. The equivalent condition is $\text{dim}\mathcal{R}(A,B)=n$.

\begin{definition}
	Let $\K$ be a field and let $A \in \K^{n \times n}$, $B \in \K^{n \times m}$, $n,m \in \N$. The pair $(A,B)$ is \termdef{controllable} if $\textnormal{dim}\mathcal{R}(A,B)=n$.
\end{definition}

\subsection{Continuous-time systems}

We will now show that the condition for \textit{discrete-time systems} is also characterizing for \textit{continuous-time systems}. For this we have to express solution of such system using matrices $A^iB$ for $i\in \N_0$. 

\begin{definition}
	Let $X$ be real or complex square matrix. The exponential of $X$, denoted by $e^X$, is the square matrix of same dimensions given by the power series $$e^{X}=\sum _{k=0}^{\infty}\frac{1}{k!}X^{k}$$
	where $X^0$ is defined to be the identity matrix $I$ with the same dimensions as $X$.
\end{definition}

We will need few properties of this notion which we will form in the next claim. 

\begin{remark}
	Let $A$, $B$ and $X$ be real or complex square matrices of same dimensions. Then 
	\begin{enumerate}
		\item $e^X$ converges for any matrix $X$.
		\item $\frac{d}{dt}e^{Xt}=Xe^{Xt}$, for $t \in \R$
		\item $AB = BA \Rightarrow e^{At}B = Be^{At}$, for $t \in \R$
	\end{enumerate}
\end{remark}

\begin{proof}
	\begin{enumerate}
		\item We want to show that the partial sums $S_N=\sum^N_{k=0}\frac{1}{k!}X^k$ converge to $e^X$. This can be shown by choosing any matrix norm satisfying $||AB||<||A||\cdot||B||$ and writing $$||e^X-S_N||=||\sum^\infty_{k=N+1}\frac{1}{k!}X^k||=\sum^\infty_{k=N+1}\frac{1}{k!}||X||^k$$ where right side approaches zero since $\sum^\infty_{k=0}\frac{||A||^k}{k!}$ converges.

		\item Thanks to the convergence we have $$\frac{d}{dt}e^{Xt}=\frac{d}{dt}\sum^\infty_{k=0}\frac{t^k}{k!}X^{k}=\sum^\infty_{k=0}\frac{t^k}{k!}X^{k+1}=X\sum^\infty_{k=0}\frac{t^k}{k!}X^{k}=Xe^{Xt}$$

		\item From definition follows $$e^{At}B=\sum^\infty_{k=0}\frac{t^k}{k!}A^{k}B\stackrel{AB=BA}{=}\sum^\infty_{k=0}\frac{t^k}{k!}BA^{k}=B\sum^\infty_{k=0}\frac{t^k}{k!}A^{k}=Be^{At}$$ 
	\end{enumerate}
\end{proof}

Now we utilize matrix exponential in solving system of inhomogeneous linear system $\dot{x}(t)=Ax(t)+Bu(t)$. By left multiplying it by $e^{-tA}$ we get
\begin{align*}
	e^{-tA}\dot{x}(t)-e^{-tA}Ax(t) &=e^{-tA}Bu(t) \\
	\frac{d}{dt} (e^{-tA}x(t)) &=e^{-tA}Bu(t) 
\end{align*}
Note we used $-AA=A(-A)\Rightarrow e^{-tA}A=Ae^{-tA}$. After integrating both sides with respect to $t$ on interval $(t_0,t_1)$ we have 
\begin{align*}
	[e^{-tA}x(t)]^{t_1}_{t_0}&=\int^{t_1}_{t_0}e^{-tA}Bu(t)dt \\
	e^{-t_1A}x(t_1)-e^{-t_0A}x(t_0)&=\int^{t_1}_{t_0}e^{-tA}Bu(t)dt \\
	x(t_1)&=e^{(t_1-t_0)A}x(t_0)+\int^{t_1}_{t_0}e^{(t_1-t)A}Bu(t)dt
\end{align*}

Now it is clear that in system where $x(0)=\nullvector$ can every state in time $t\in \R^+$ be expressed as $$x(t)=\int^t_0 e^{(t-s)A}Bu(s)ds$$

\begin{theorem}
	The $n$-dimensional continuos-time linear system is controllable if and only if $\text{dim}\mathcal{R}(A,B)=n$.
\end{theorem}

\begin{proof}
	From discussion above we have $$x(t)=\int^t_0e^{(t-s)A}Bu(s)ds=\int^t_0\sum^\infty_{k=0}\frac{(t-s)^k}{k!}A^kBu(s)$$ we can see that $$x(t) \in \text{Im}(B|AB|\ldots|A^kB|\ldots)\subseteq \text{Im}(B|AB|\ldots|A^{n-1}B)=\mathcal{R}(A,B)$$ 
	
	If the system is controllable then $x(t)$ can be equal to any of the vectors of an arbitrary basis of $\K^n$. Therefore we know that $n$ linearly independent vectors belong into $\mathcal{R}(A,B)$ and naturally it follows $\text{dim}\mathcal{R}(A,B)=n$.

	Conversely, if dimension of reachable space is less than $n$, then any basis vector of complement space to $\mathcal{R}(A,B)$ cannot be equal to $x(t)$. Therefore the system is not controllable.
\end{proof}

\subsection{Decomposition theorem}

If $(A,B)$ are not controllable then there exists subspace of our state space that is not affected by our input. This can be shown using following theorem.

\begin{theorem}
	\label{theorem:decomp}
	Assume that $(A,B)$ is not controllable. Let $\text{dim}\mathcal{R}(A,B)=r<n$. Then there exists invertible $n\times n$ matrix $T$ over $\K$ such that the matrices $\widetilde{A}:=T^{-1}AT$ and $\widetilde{B}:=T^{-1}B$ have the block structure 
	\begin{equation*}
		\widetilde{A}=
		\begin{pmatrix}
			A_1 & A_2 \\
			0   & A_3 
		\end{pmatrix}
		\qquad
		\widetilde{B}=
		\begin{pmatrix}
			B_1  \\
			0
		\end{pmatrix}
	\end{equation*}
	where $A_1$ is $r \times r$ and $B_1$ is $r \times m$.
\end{theorem}

\begin{proof}
	Let $\mathcal{S}$ be any subspace that $$\mathcal{R}(A,B)\oplus\mathcal{S}=\K^n.$$ Let $\{v_1,\ldots,v_r\}$ be the basis of $\mathcal{R}(A,B)$ and $\{w_1,\ldots,w_{n-r}\}$ be the basis of $\mathcal{S}$, then we put $K=(v_1,\ldots,v_r,w_1,\ldots,w_{n-r})$ as the basis of $\K^n$ and we put $$T:=(v_1|\ldots|v_r|w_1|\ldots|w_{n-r})=[\text{id}]^K_C$$ where $C$ is the canonical basis and $[\text{id}]^K_C$ is the transition matrix from basis $K$ to basis $C$. We have $\text{Im}T=\K^n$ therefore $T$ is an invertible matrix. It holds $$\widetilde{B}=T^{-1}B=([\text{id}]^K_C)^{-1}B=[\text{id}]^C_KB$$ We know that $\text{Im}B\subseteq\mathcal{R}(A,B)$ therefore every column of matrix $B$ can be uniquely expressed as linear combination of vectors in basis $K$. From our choice of $T$ we can clearly see that $\widetilde{B}$ will be of the desired form.
	
	As for $\widetilde{A}$ we have $$\widetilde{A}=T^{-1}AT=[\text{id}]^C_KA[\text{id}]^K_C$$ From the fact that $\mathcal{R}(A,B)$ is $A$-invariant it follows that $$AT=(u_1|\ldots|u_r|z_1|\ldots|z_{n-r})$$ where $u_i \in \mathcal{R}(A,B)$ and $z_i \in \K^i$. Therefore, when we express these vectors in the basis $K$ (by left multiplying $AT$ by $T^{-1}=[\text{id}]^C_K$) we get the required structure of matrix $\widetilde{A}$.
\end{proof}

We achieved the new form of matrices $A$ and $B$ by changing the basis of our state space. We define relation between $(A,B)$ and $(\widetilde{A},\widetilde{B}).$

\begin{definition}
	Let $(A,B)$ and $(\widetilde{A},\widetilde{B})$ be pairs as above. Then $(A,B)$ \termdef{is similar to} $(\widetilde{A},\widetilde{B})$, denoted $$(A,B) \sim (\widetilde{A},\widetilde{B})$$ if there exists invertible matrix $T$ for which it holds that $$\widetilde{A}=T^{-1}AT\quad and\quad\widetilde{B}=T^{-1}B$$
\end{definition}

We can interpret the decomposition as follows. Consider our system $\dot{x}(t)=Ax(t)+Bu(t)$. By changing the basis by putting $x(t)=Ty(t)$ we get $$T\dot{y}(t)=ATy(t)+Bu(t)$$ which we can write as $$\dot{y}(t)=T^{-1}ATy(t)+T^{-1}Bu(t)=\widetilde{A}y(t)+\widetilde{B}u(T)$$ Which gives us 
\begin{alignat*}{2}
	\dot{y}_1(t)&=A_1y_1(t)+&A_2y_2(t)&+B_1u_1(t) \\
	\dot{y}_2(t)&=&A_3y_2(t)&
\end{alignat*}
where $y_1(t)$ is the first $r$ elements of $y(t)$, $y_2(t)$ is the other $n-r$ elements of $y(t)$ and $u_1(t)$ is the first $r$ elements of $u(t)$. It is clear that we cannot control $\dot{y}_2$ by any means. 

It is also true that $(A_1,B_1)$ is a controllable pair which we will state in a lemma.

\begin{lemma}
	The pair $(A_1,B_1)$ is controllable.
\end{lemma}

\begin{proof}
	We know that $\text{dim}\mathcal{R}(A,B)=r$. We desire $\text{dim}\mathcal{R}(A_1,B_1)=r$. We will show that $\mathcal{R}(\widetilde{A},\widetilde{B})=\mathcal{R}(A,B)$ and that each vector in $\mathcal{R}(\widetilde{A},\widetilde{B})$ has its last $n-r$ elements equal to 0 and that $\mathcal{R}(\widetilde{A},\widetilde{B})$ restricted on its first $r$ coordinates is equal to $\mathcal{R}(A_1,B_1)$. 
	\begin{align*}
		\mathcal{R}(\widetilde{A},\widetilde{B})&=\text{Im}(\widetilde{A}^{n-1}\widetilde{B}|\ldots|\widetilde{A}\widetilde{B}|\widetilde{B}) \\
		&=\text{Im}((T^{-1}AT)^{n-1}T^{-1}B|\ldots|T^{-1}ATT^{-1}B|T^{-1}B) \\
		&=\text{Im}(T^{-1}A^{n-1}B|\ldots|T^{-1}AB|T^{-1}B) \\
		&=T^{-1}\cdot\text{Im}(A^{n-1}B|\ldots|AB|B) \\
		&=T^{-1}\cdot\mathcal{R}(A,B)
	\end{align*}
	Since $T$ is an invertible matrix, which preserves linear independence, we have $$\text{dim}\mathcal{R}(\widetilde{A},\widetilde{B})=\text{dim}(T^{-1}\mathcal{R}(A,B))=\text{dim}(\mathcal{R}(A,B))=r$$

	Now let us focus on the structure of $\mathcal{R}(\widetilde{A},\widetilde{B})$: We know that last $n-r$ rows of $\widetilde{B}$ are $\nullvector$. Also because of structure of $\widetilde{A}$ we have for an arbitrary matrix $X\in\K^{r\times m}$ that 
	\begin{equation*}
		\widetilde{A}
		\begin{pmatrix}
			X \\
			0
		\end{pmatrix}
		=
		\begin{pmatrix}
			A_1 & A_2 \\
			  0 & A_3
		\end{pmatrix}
		\begin{pmatrix}
			X \\
			0
		\end{pmatrix}
		=
		\begin{pmatrix}
			A_1X \\
			0
		\end{pmatrix}
	\end{equation*}
	where again are the last $n-r$ rows equal to $\nullvector$. Therefore we see that for any positive integer $k$ we have 
	\begin{equation*}
		\widetilde{A}^k\widetilde{B}=
		\begin{pmatrix}
			A_1^{k}B_1 \\
			0
        \end{pmatrix}
        ,A_1^kB_1\in\K^{r\times r}
    \end{equation*}
    It follows
    \begin{equation*}
        \mathcal{R}(\widetilde{A},\widetilde{B})=
        \begin{pmatrix}[c|c|c|c]
            \begin{pmatrix}
                A_1^{n-1}B_1 \\
                0 
            \end{pmatrix}
            & \ldots &
            \begin{pmatrix}
                A_1B_1 \\
                0 
            \end{pmatrix}
            &
            \begin{pmatrix}
                B_1 \\
                0 
            \end{pmatrix}
        \end{pmatrix}
    \end{equation*}
	
	From Cayle-Hemilton theorem we therefore again have that the restriction to first $r$ coordinates (those which are not 0) of $\mathcal{R}(\widetilde{A},\widetilde{B})$ are equal to $\mathcal{R}(A_1,B_1)$. Finally, it follows that $$\text{dim}\mathcal{R}(A_1,B_1)=\text{dim}\mathcal{R}(\widetilde{A},\widetilde{B})=\text{dim}\mathcal{R}(A,B)=r$$
\end{proof}

Now we can see that the decomposition from Theorem \ref{theorem:decomp} decomposes the matrix $A$ into ``controllable'' and ``uncontrollable'' parts $A_1$ and $A_3$ respectively.

We also see that 
\begin{align*}
	\chi_{\widetilde{A}}&=\text{det}(sI-\widetilde{A})=\text{det}(sI-T^{-1}AT) \\
	&=\text{det}(sT^{-1}IT-T^{-1}AT)=\text{det}(T^{-1}(sI-A)T) \\
	&=(\text{det}T)^{-1}\text{det}(sI-A)\text{det}T=\text{det}(sI-A) \\
	&=\chi_A
\end{align*}
therefore it holds $$\chi_A=\chi_{A_1}\chi_{A_3}$$ 

\begin{definition}
	The characteristic polynomial of matrix $A$ splits into \termdef{controllable} and \termdef{uncontrollable parts} with respect to pair $(A,B)$ which we denote by $\chi_c$ and $\chi_u$ respectively. We define these polynomials as $$\chi_c=\chi_{A_1} \qquad \chi_u=\chi_{A_3}$$ In case $r=0$ we put $\chi_c=1$ and in case $r=n$ we put $\chi_u=1$.
\end{definition}